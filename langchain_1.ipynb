{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caochunqin/Desktop/githomework/milestone2_pt2/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img2text(url):\n",
    "    img_to_text_pipe = pipeline(\"image-to-text\", model=\"Salesforce/blip-image-captioning-large\")\n",
    "    text = img_to_text_pipe(url)[0][\"generated_text\"]\n",
    "    # print(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "/Users/caochunqin/Desktop/githomework/milestone2_pt2/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bookshelves with many different colored books on them in a library\n"
     ]
    }
   ],
   "source": [
    "scenario = img2text(\"library.jpg\")\n",
    "print(scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.2,\n",
    "    max_tokens=200,\n",
    "    timeout=None,\n",
    "    max_retries=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert short story teller. Using a simple narrative you generate story in less than 100 words based on the given scenario.\",\n",
    "        ),\n",
    "        (\"human\", \"{scenario_lang}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the quiet library, sunlight streamed through tall windows, casting rainbows on the bookshelves. Each book, a different color, whispered secrets of distant lands and forgotten times. A young girl, eyes wide with wonder, reached for a vibrant blue spine. As she opened it, the room seemed to hold its breath. The pages rustled softly, revealing tales of ocean voyages and mythical creatures. She smiled, feeling the magic of the library envelop her. In that moment, surrounded by a kaleidoscope of stories, she knew she had found her own adventure waiting to be written.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "chain.invoke({\n",
    "    \"scenario_lang\" : scenario\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textGeneration_langChain(msg,type):\n",
    "    \"\"\"\n",
    "    msg is the scenario for the story from the pic (hugging face model output);\n",
    "    type is the genre of the story- Horror, Fantasy, Adventure, Comedy, Mystery, Romance\n",
    "    \"\"\"\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-4o\",\n",
    "        temperature=0.2,\n",
    "        max_tokens=200,\n",
    "        timeout=None,\n",
    "        max_retries=2\n",
    "    )\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are an expert short {story_type} story teller. Using a simple narrative you generate {story_type} story in less than 100 words based on the given scenario.\",\n",
    "            ),\n",
    "            (\n",
    "                \"human\", \n",
    "                \"{scenario_lang}\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    out_message = chain.invoke({\n",
    "        \"story_type\" : type,\n",
    "        \"scenario_lang\" : msg,\n",
    "    })\n",
    "\n",
    "    return out_message\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the dimly lit library, the bookshelves stood like sentinels, their spines a kaleidoscope of colors. As I reached for a crimson tome, it slipped from my grasp, landing with a hollow thud. The room seemed to exhale, and the books began to whisper, their voices a cacophony of forgotten secrets. I backed away, but the shelves loomed closer, the colors swirling into a vortex of madness. The whispers grew louder, a chant of my name, pulling me in. The last thing I saw was the crimson book opening, its pages blank, waiting to write my story.\n"
     ]
    }
   ],
   "source": [
    "story = textGeneration_langChain(scenario,\"Horror\")\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runModels_langchain(url, type):\n",
    "    scenario = img2text(url)\n",
    "    story = textGeneration_langChain(scenario,type)\n",
    "    return([scenario,story])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "/Users/caochunqin/Desktop/githomework/milestone2_pt2/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the quiet library, a mischievous librarian named Mr. Page had a secret hobby: color-coding chaos. One day, he rearranged the books so that each shelf formed a rainbow. Patrons were baffled. \"Excuse me,\" a woman asked, \"where's 'War and Peace'?\" Mr. Page grinned, \"Under 'W' for 'Wow, that's colorful!'\" \n",
      "\n",
      "A man searching for a mystery novel found himself in the romance section, confused by the pink spines. \"Is this a clue?\" he wondered aloud. Mr. Page winked, \"Only if you love a good mystery!\" \n",
      "\n",
      "And so, the library became a place where finding a book was the real adventure.\n"
     ]
    }
   ],
   "source": [
    "(caption, story) = runModels_langchain(\"library.jpg\",\"Comedy\")\n",
    "print(story)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
